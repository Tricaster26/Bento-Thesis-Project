{"name": "cache", "code": "import hashlib\nimport socks\nimport requests\nfrom urllib.parse import urlparse\nimport urllib\nimport time\nimport socket\nfrom cacheout import lru\nimport socks\nimport signal\nimport sys\nimport os\nimport signal\n\ncurr_url = None\ncurr_protocol = None\n\nl = lru.LRUCache(maxsize=256, ttl=86400, timer=time.time, default=None) # use this cache to use least recently used cache policy (websites are cached in memory)\n# ttl is num seconds in a day\n\nserver = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\nserver.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)\nserver.bind(('', 8009))\nserver.listen(10) # we can change the backlog if necessary\n\n\ndef init_cache():\n    if(os.path.exists('./cache')):\n        for filename in os.listdir(\"./cache\"):\n            name = \"./cache/\" + filename\n            f = open(name, 'rb')\n            l.set(filename, f.read())\n            f.close()\n    else:\n        os.mkdir('./cache')\n\ninit_cache()\n\ndef signal_handler(sig, frame):\n    print(\"detected sigint\")\n    for h in l:\n        filename = \"./cache/\" + h\n        f = open(filename, 'wb+')\n        f.write(l.get(h))\n    l.clear()\n    sys.exit(0)\n\ndef fetch_file(parsed_url: urllib.parse.ParseResult) -> bytes:\n    h = hashlib.sha256()\n    filename = None\n    global curr_url\n    global curr_protocol\n    if(curr_url != None):\n        filename = curr_url + parsed_url.path\n    else:\n        filename = parsed_url.netloc + parsed_url.path\n    assert(filename != None)\n    h.update(filename.encode())\n    file_digest = h.hexdigest()\n    content = l.get(file_digest)\n    if(content == None):\n        if(curr_url != None):\n            print()\n            print(\"curr_url is: \" + curr_url)\n            print()\n        if(parsed_url.netloc == ''):\n            url = curr_protocol + '://' + curr_url + '/' + parsed_url.path\n        else:\n            url = curr_protocol + '://' + parsed_url.netloc + parsed_url.path\n        print(url)\n        # set up tor connection\n        session = requests.session()\n        session.proxies = {}\n        session.proxies['http'] = 'socks5h://localhost:9050'\n        session.proxies['https'] = 'socks5h://localhost:9050'\n\n        content = session.get(url).content\n        session.close()\n        print(\"saving to cache\")\n        print(file_digest)\n        l.set(file_digest, content)\n        print()\n        print(\"returning content\")\n        return content\n    return content\n\n\ndef cproxy():\n    print(\"starting cproxy\")\n    signal.signal(signal.SIGINT, signal_handler)\n    while(True):\n        client, addr = server.accept()\n        print(addr)\n        print()\n        request = client.recv(1024)\n        fields = request.split()\n        print(fields)\n        print()\n        if(len(fields) <= 1):\n            client.close()\n            continue\n        method = fields[0]\n        filename = fields[1][1:] # has format -> /www.google.com\n        print()\n        print(\"filename is: \" + filename.decode('utf-8'))\n        print()\n        parsed = urlparse(filename.decode('utf-8'))\n        print(parsed)\n        if(parsed.netloc != ''):\n             global curr_url\n             global curr_protocol\n             curr_url = parsed.netloc\n             curr_protocol = parsed.scheme\n        content = fetch_file(parsed)\n        if(content):\n            print(\"sending 200\")\n            print()\n            client.sendall(b'HTTP/1.0 200 OK\\r\\n\\r\\n' + content)\n        else:\n            print(\"sending 404\")\n            print()\n            client.sendall(b'HTTP/1.0 404 NOT FOUND\\r\\n\\r\\n File Not Found')\n        l.evict()\n        client.close()"}